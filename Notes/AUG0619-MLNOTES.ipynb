{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>August 6, 2019 Coursera Course Week 3</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notes: Catch up\n",
    "\n",
    "Stephan and Sushat(?) spark notes<br>\n",
    "supervised-labeled data, know values, generalize<br>\n",
    "unsupervisedunlabeled data, have the data, no known predicitions-clustering one technique to use<br>\n",
    "value trying to predict is what is labeled or unlabled.<br>\n",
    "unsup example- class and want to break them up(learning needs, etc) dont know what the best is, make clusters.<br>\n",
    "supervised- housing data, know the prices you want to predict.<br>\n",
    "Unsupervised unlikely to be covered in the course much- idea is to continue learning.<br>\n",
    "logistic reg-classification <br>\n",
    "example: test scores into two groups.<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "New Stuff-Problem of Overfitting\n",
    "\n",
    "Overfitting: comes when we have too many features, model fits training set too closely, and will fail on new examples.\n",
    "Logistic regression will build a boundary line that may closely fit the data.\n",
    "Addressing it-two options. \n",
    "\n",
    "1. Reduce features\n",
    "\n",
    "manually find important ones, or through Model selection algos.\n",
    "\n",
    "Downside: may remove important features\n",
    "\n",
    "2. Regularization\n",
    "\n",
    "Keep all the features, regularizie them. https://datanice.github.io/machine-learning-101-what-is-regularization-interactive.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Regularization:\n",
    "Make the parameters small for the thetas. Less prone to overfitting.\n",
    "\n",
    "take cost function, modify to add term at end, called regularization parameter(lambda).\n",
    "\n",
    "lambda- penalization of nth degree, sum over j=1 to n.\n",
    "\n",
    "penalize- one thing so big, others small, make them more standard. \n",
    "\n",
    "If lambda too large will underfit.\n",
    "\n",
    "As #examples grows, they will approach each other."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next Video- lots of math. Regularization with Linear Regression<br>\n",
    "Quiz- Answer is 1-alpha(Lambda/m)<1<br>\n",
    "Why? Lambda/m must be positive, alpha may be small, 1-anything<1.<br>\n",
    "non-invertibility- feel free to skip as it is optional.<br>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
